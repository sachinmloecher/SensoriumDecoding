{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72115693-ec37-4340-9e0b-3ed821490895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/SensoriumDecoding\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92c4f1d-f3a1-47eb-85fe-bf2c90078cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn.functional import mse_loss\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import typing as t\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import src.utils.data as data\n",
    "from src.utils.data import load_args, get_training_ds, DataLoader\n",
    "from src.utils.losses import get_criterion\n",
    "import src.utils.utils as utils\n",
    "from src.utils.utils import Logger, Scheduler\n",
    "from src.models.model import get_model, FullModel\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.output_dir = \"runs/DNN/first_run\"\n",
    "args = Args()\n",
    "load_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d00a33-f85b-4ef2-bfbe-3311f9bf163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather(result: t.Dict[str, t.List[torch.Tensor]]):\n",
    "    return {k: torch.sum(torch.stack(v)).cpu() for k, v in result.items()}\n",
    "\n",
    "def vstack(tensors: t.List[torch.Tensor]):\n",
    "    return torch.vstack(tensors).cpu()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    \"\"\"Metrics to compute as part of training and validation step\"\"\"\n",
    "    batch_size, h, w = y_true.size()\n",
    "    # Reshape tensors to match the required format for calculations\n",
    "    y_true = y_true.view(batch_size, -1)\n",
    "    y_pred = y_pred.view(batch_size, -1)\n",
    "    # Pixel wise correlation\n",
    "    correlations = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        correlation, _ = pearsonr(y_true[:, i].cpu(), y_pred[:, i].cpu())\n",
    "        correlations.append(correlation)\n",
    "    correlation = torch.tensor(correlations).mean().item()\n",
    "    # RMSE\n",
    "    rmse = torch.sqrt(mse_loss(y_true, y_pred))\n",
    "    # SSIM\n",
    "    ssim_score = torch.tensor([ssim(y_true[i].cpu().numpy(), y_pred[i].cpu().numpy(), data_range=4) for i in range(batch_size)]).mean()\n",
    "    return {\n",
    "        \"correlation\": correlation,\n",
    "        \"rmse\": rmse.item(),\n",
    "        \"ssim\": ssim_score.item(),\n",
    "    }\n",
    "\n",
    "def train_step(\n",
    "    mouse_id: str,\n",
    "    batch: t.Dict[str, torch.Tensor],\n",
    "    model: FullModel,\n",
    "    optimizer: torch.optim,\n",
    "    criterion: torch.nn.Module,\n",
    "    update: bool,\n",
    "    micro_batch_size: int,\n",
    "    device: torch.device = \"cpu\"\n",
    "):\n",
    "    # Define single train step with microbatching\n",
    "    model.to(device)\n",
    "    batch_size = batch[\"image\"].size(0)\n",
    "    result = {\"loss/loss\": []}\n",
    "    for micro_batch in data.micro_batching(batch, micro_batch_size):\n",
    "        y_true = micro_batch[\"image\"].to(device)\n",
    "        y_pred = model(\n",
    "            x=micro_batch[\"response\"].to(device),\n",
    "            mouse_id=mouse_id,\n",
    "            behaviours=micro_batch[\"behavior\"].to(device),\n",
    "            pupil_centers=micro_batch[\"pupil_center\"].to(device)\n",
    "        )\n",
    "        y_pred = y_pred.view(y_true.size(0), 36, 64)\n",
    "        loss = criterion(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            mouse_id=mouse_id,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        loss.backward()\n",
    "        result[\"loss/loss\"].append(loss.detach())\n",
    "    if update:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return gather(result)\n",
    "\n",
    "def train(\n",
    "    args,\n",
    "    ds: t.Dict[str, DataLoader],\n",
    "    model: FullModel,\n",
    "    optimizer: torch.optim,\n",
    "    criterion: torch.nn.Module,\n",
    "    epoch: int\n",
    "):\n",
    "    mouse_ids = list(ds.keys())\n",
    "    results = {mouse_id: {} for mouse_id in mouse_ids}\n",
    "    ds = data.CycleDataloaders(ds)\n",
    "    update_frequency = len(mouse_ids)\n",
    "    model.train(True)\n",
    "    optimizer.zero_grad()\n",
    "    for i, (mouse_id, mouse_batch) in tqdm(\n",
    "        enumerate(ds), desc=\"Train\", total=len(ds), disable=args.verbose < 2\n",
    "    ):\n",
    "        result = train_step(\n",
    "            mouse_id=mouse_id,\n",
    "            batch=mouse_batch,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            update=(i + 1) % update_frequency == 0,\n",
    "            micro_batch_size=args.micro_batch_size,\n",
    "            device=args.device\n",
    "        )\n",
    "        utils.update_dict(results[mouse_id], result)\n",
    "    return utils.log_metrics(results, epoch=epoch, mode=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_step(\n",
    "    mouse_id: str,\n",
    "    batch: t.Dict[str, torch.Tensor],\n",
    "    model: FullModel,\n",
    "    criterion: torch.nn.Module,\n",
    "    micro_batch_size: int,\n",
    "    device: torch.device = \"cpu\"\n",
    "):\n",
    "    model.to(device)\n",
    "    batch_size = batch[\"image\"].size(0)\n",
    "    result = {\"loss/loss\": []}\n",
    "    targets, predictions = [], []\n",
    "    for micro_batch in data.micro_batching(batch, micro_batch_size):\n",
    "        y_true = micro_batch[\"image\"].to(device)\n",
    "        y_pred = model(\n",
    "            x=micro_batch[\"response\"].to(device),\n",
    "            mouse_id=mouse_id,\n",
    "            behaviours=micro_batch[\"behavior\"].to(device),\n",
    "            pupil_centers=micro_batch[\"pupil_center\"].to(device),\n",
    "        )\n",
    "        y_pred = y_pred.view(y_true.size(0), 36, 64)\n",
    "        loss = criterion(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            mouse_id=mouse_id,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        result[\"loss/loss\"].append(loss)\n",
    "        targets.append(y_true)\n",
    "        predictions.append(y_pred)\n",
    "    return gather(result), vstack(targets), vstack(predictions)\n",
    "\n",
    "\n",
    "def validate(\n",
    "    args,\n",
    "    ds: t.Dict[str, DataLoader],\n",
    "    model: FullModel,\n",
    "    criterion: torch.nn.Module,\n",
    "    epoch: int\n",
    "):\n",
    "    model.train(False)\n",
    "    results = {}\n",
    "    with tqdm(desc=\"Val\", total=len(ds), disable=args.verbose < 2) as pbar:\n",
    "        for mouse_id, mouse_ds in ds.items():\n",
    "            mouse_result, y_true, y_pred = {}, [], []\n",
    "            for batch in mouse_ds:\n",
    "                result, targets, predictions = validation_step(\n",
    "                    mouse_id=mouse_id,\n",
    "                    batch=batch,\n",
    "                    model=model,\n",
    "                    criterion=criterion,\n",
    "                    micro_batch_size=args.micro_batch_size,\n",
    "                    device=args.device\n",
    "                )\n",
    "                utils.update_dict(mouse_result, result)\n",
    "                y_true.append(targets)\n",
    "                y_pred.append(predictions)\n",
    "                pbar.update(1)\n",
    "            y_true, y_pred = vstack(y_true), vstack(y_pred)\n",
    "            mouse_result.update(compute_metrics(y_true=y_true, y_pred=y_pred))\n",
    "            results[mouse_id] = mouse_result\n",
    "            del y_true, y_pred\n",
    "    return utils.log_metrics(results, epoch=epoch, mode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1280c82e-e1d1-424f-b402-74597ae6e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, args, epoch, history):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history\n",
    "    }\n",
    "    torch.save(checkpoint, args.output_dir + \"/test.pt\")\n",
    "\n",
    "def load_checkpoint(args, optimizer, scheduler):\n",
    "    checkpoint = torch.load(args.output_dir + \"/test.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    history = checkpoint['history']\n",
    "    return model, optimizer, scheduler, epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e7c3e41-be2b-4825-98d2-b579264f542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(args).to(args.device)  # Define the model architecture\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_correlation\": []}\n",
    "save_checkpoint(model, optimizer, scheduler, args, 1, history)\n",
    "model2, optimizer2, scheduler2, start_epoch, history = load_checkpoint(args, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91fe1da1-dd59-4393-af57-cd82e88226e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(torch.allclose(model.state_dict()[key], model2.state_dict()[key]) for key in model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f52d5dd-63d6-4bd9-ad1f-7d0c3fcf6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, args, epoch, history):\n",
    "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\", f\"epoch_{epoch}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(checkpoint_dir, \"model.pt\")\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    # Save optimizer\n",
    "    optimizer_path = os.path.join(checkpoint_dir, \"optimizer.pt\")\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    \n",
    "    # Save scheduler\n",
    "    scheduler_path = os.path.join(checkpoint_dir, \"scheduler.pt\")\n",
    "    torch.save(scheduler.state_dict(), scheduler_path)\n",
    "    \n",
    "    # Save other necessary components\n",
    "    history_path = os.path.join(checkpoint_dir, \"history.pt\")\n",
    "    torch.save(history, history_path)\n",
    "\n",
    "def load_checkpoint(args, optimizer, scheduler):\n",
    "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\")\n",
    "    \n",
    "    # Get list of checkpoint folders\n",
    "    checkpoint_folders = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
    "    \n",
    "    if not checkpoint_folders:\n",
    "        raise FileNotFoundError(\"No checkpoint folders found.\")\n",
    "    \n",
    "    # Get the latest epoch folder\n",
    "    latest_epoch_folder = max(checkpoint_folders, key=lambda x: int(x.split(\"_\")[-1]))\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, latest_epoch_folder)\n",
    "    \n",
    "    # Load model\n",
    "    model_path = os.path.join(checkpoint_dir, \"model.pt\")\n",
    "    model = torch.load(model_path)\n",
    "    model.train(True)\n",
    "    \n",
    "    # Load optimizer\n",
    "    optimizer_path = os.path.join(checkpoint_dir, \"optimizer.pt\")\n",
    "    optimizer_state_dict = torch.load(optimizer_path)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)\n",
    "    \n",
    "    # Load scheduler\n",
    "    scheduler_path = os.path.join(checkpoint_dir, \"scheduler.pt\")\n",
    "    scheduler_state_dict = torch.load(scheduler_path)\n",
    "    scheduler.load_state_dict(scheduler_state_dict)\n",
    "    \n",
    "    # Extract epoch from folder name\n",
    "    epoch = int(latest_epoch_folder.split(\"_\")[-1])\n",
    "    \n",
    "    # Load history\n",
    "    history_path = os.path.join(checkpoint_dir, \"history.pt\")\n",
    "    history = torch.load(history_path)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "653d2f5f-a2af-4191-9d68-463229b4a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.resume = False\n",
    "model = get_model(args).to(args.device)  # Define the model architecture\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_correlation\": []}\n",
    "save_checkpoint(model, optimizer, scheduler, args, 1, history)\n",
    "model2, optimizer2, scheduler2, start_epoch, history = load_checkpoint(args, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66ac5d2-4bcd-479d-beeb-5b0ea79966db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(torch.allclose(model.state_dict()[key], model2.state_dict()[key]) for key in model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6b427-6eb7-48f6-8265-90acefd73005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
